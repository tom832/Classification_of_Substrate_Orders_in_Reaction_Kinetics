{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.feature_extraction.settings import ComprehensiveFCParameters\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction.feature_calculators import set_property\n",
    "import pycatch22\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_seed = 29\n",
    "\n",
    "old_class_list = [\n",
    "    '1__single', \n",
    "    # '1__single_outside', \n",
    "    '1__double',\n",
    "    '1__double_outside',\n",
    "    '1__double_inside',\n",
    "    '0__single',\n",
    "    # '0__single_outside',\n",
    "    '0__double',\n",
    "    '0__double_outside',\n",
    "    '0__double_inside',\n",
    "    'mm__single',\n",
    "    # 'mm__single_outside',\n",
    "    # 'mm__single_outside__add',\n",
    "    'mm__double',\n",
    "    'mm__double_outside',\n",
    "    'mm__double_inside',\n",
    "]\n",
    "\n",
    "old_class_short_list = [\n",
    "    '1_sgl', \n",
    "    # '1_sgl_out', \n",
    "    '1_dbl',\n",
    "    '1_dbl_out',\n",
    "    '1_dbl_in',\n",
    "    '0_sgl',\n",
    "    # '0_sgl_out',\n",
    "    '0_dbl',\n",
    "    '0_dbl_out',\n",
    "    '0_dbl_in',\n",
    "    'mm_sgl',\n",
    "    # 'mm_sgl_out',\n",
    "    'mm_dbl',\n",
    "    'mm_dbl_out',\n",
    "    'mm_dbl_in',\n",
    "]\n",
    "\n",
    "old_class2new_class = {\n",
    "    '1__single': 'first', \n",
    "    # '1__single_outside': 'first', \n",
    "    '1__double': 'second',\n",
    "    '1__double_outside': 'second',\n",
    "    '1__double_inside': 'second',\n",
    "    '0__single': 'zero',\n",
    "    # '0__single_outside': 'zero',\n",
    "    '0__double': 'first',\n",
    "    '0__double_outside': 'first',\n",
    "    '0__double_inside': 'first',\n",
    "    'mm__single': 'mm_1',\n",
    "    # 'mm__single_outside': 'mm_1_out',\n",
    "    # 'mm__single_outside__add': 'mm_1_out',\n",
    "    'mm__double': 'mm_2',\n",
    "    'mm__double_outside': 'mm_2',\n",
    "    'mm__double_inside': 'mm_2',\n",
    "}\n",
    "\n",
    "class_list = [\n",
    "    'zero',\n",
    "    'first',\n",
    "    'second',\n",
    "    'mm_1',\n",
    "    # 'mm_1_out',\n",
    "    'mm_2',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(cat_conc='norm_cat', random_seed=42):\n",
    "    data_root = os.path.join(f'../Ye/previous/{cat_conc}/')\n",
    "    task_names = [\n",
    "        '1__single', \n",
    "        # '1__single_outside', \n",
    "        '1__double',\n",
    "        '1__double_outside',\n",
    "        '1__double_inside',\n",
    "        '0__single',\n",
    "        # '0__single_outside',\n",
    "        '0__double',\n",
    "        '0__double_outside',\n",
    "        '0__double_inside',\n",
    "        'mm__single',\n",
    "        # 'mm__single_outside',\n",
    "        # 'mm__single_outside__add',\n",
    "        'mm__double',\n",
    "        'mm__double_outside',\n",
    "        'mm__double_inside',\n",
    "    ]\n",
    "    for task_name in tqdm(task_names):\n",
    "        task_data_root = os.path.join(data_root, task_name)\n",
    "        save_root = os.path.join('data', cat_conc, 'raw')\n",
    "        if not os.path.exists(save_root):\n",
    "            os.makedirs(save_root)\n",
    "\n",
    "        class_name = old_class2new_class[task_name]\n",
    "        new_class_data_path = os.path.join(save_root, f'{class_name}__all.csv')\n",
    "        if os.path.exists(new_class_data_path):\n",
    "            df_new_class = pd.read_csv(new_class_data_path)\n",
    "        else:\n",
    "            df_new_class = pd.DataFrame()\n",
    "\n",
    "        data_column = ['id']\n",
    "        for i in range(30):\n",
    "            data_column += [f's{i}']\n",
    "        for i in range(30):\n",
    "            data_column += [f'p{i}']\n",
    "        data = pd.DataFrame(columns=data_column)\n",
    "\n",
    "        for file in os.listdir(task_data_root):\n",
    "            if file.endswith('.json'):\n",
    "                with open(os.path.join(task_data_root, file), 'r') as f:\n",
    "                    task_data = json.load(f)\n",
    "                \n",
    "                id = f'{class_name}-{task_name}-{file[:-5]}'\n",
    "                data.loc[data.shape[0]] = [id] + task_data['s'] + task_data['p']\n",
    "\n",
    "        # data.to_csv(os.path.join(save_root, f'{task_name}__all.csv'), index=False)\n",
    "        data_noDup = data.drop_duplicates('id')\n",
    "        assert data_noDup.shape[0] == data.shape[0]\n",
    "        print(f'{task_name}: {data.shape[0]}')\n",
    "\n",
    "        df_new_class = pd.concat([df_new_class, data], ignore_index=True)\n",
    "        df_new_class.to_csv(new_class_data_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "# def check_A_a_B(data):\n",
    "#     A = data['A']\n",
    "#     a = data['a']\n",
    "#     B = data['B']\n",
    "#     return (a+B)/A < 0.005\n",
    "\n",
    "# def save_data_file(task_name, save_root='data', data_size=10000, random_seed=0):\n",
    "#     # print('task_name:', task_name)\n",
    "#     data = collect_data(task_name)\n",
    "#     data_noDup = data.drop_duplicates()\n",
    "#     assert data_noDup.shape[0] == data.shape[0]\n",
    "#     data.to_csv(os.path.join(save_root, f'{task_name}_data_all.csv'), index=False)\n",
    "#     data = data.sample(n=data_size, random_state=random_seed)\n",
    "#     data.to_csv(os.path.join(save_root, f'{task_name}_data_{data_size}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/12 [00:18<03:23, 18.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__single: 11404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2/12 [00:35<02:55, 17.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__double: 10710\n",
      "1__double_outside: 11156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 3/12 [00:53<02:39, 17.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__double_inside: 13332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 5/12 [01:37<02:23, 20.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__single: 13671\n",
      "0__double: 12214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 6/12 [01:57<02:01, 20.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__double_outside: 20369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 7/12 [02:34<02:07, 25.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__double_inside: 12902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9/12 [03:12<01:06, 22.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__single: 10742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 10/12 [03:29<00:40, 20.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__double: 10728\n",
      "mm__double_outside: 10772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 11/12 [03:46<00:19, 19.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__double_inside: 11022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [04:04<00:00, 20.38s/it]\n"
     ]
    }
   ],
   "source": [
    "save_root = 'data'\n",
    "if not os.path.exists(save_root):\n",
    "    os.makedirs(save_root)\n",
    "    \n",
    "collect_data('norm_cat', random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:00,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__single\n",
      "zero\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:00,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__single\n",
      "0__double\n",
      "0__double_outside\n",
      "0__double_inside\n",
      "first\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:00<00:00,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__double\n",
      "1__double_outside\n",
      "1__double_inside\n",
      "second\n",
      "(10000, 62)\n",
      "(10000, 62)\n",
      "mm__single\n",
      "mm_1\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__double\n",
      "mm__double_outside\n",
      "mm__double_inside\n",
      "mm_2\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def stratified_sample(df, col, n_samples):\n",
    "    return df.groupby(col, group_keys=False).apply(lambda x: x.sample(min(len(x), n_samples), random_state=random_seed)).reset_index(drop=True)\n",
    "\n",
    "data_size_for_tasks = {\n",
    "    '0__single': 10000,\n",
    "    # '0__single': 5000,\n",
    "    # '0__single_outside': 5000,\n",
    "\n",
    "    'mm__single': 10000,\n",
    "\n",
    "    # 'mm__single_outside': 10000,\n",
    "    # 'mm__single_outside__add': 200,\n",
    "\n",
    "    '1__single': 3220, \n",
    "    # '1__single_outside': 2415, \n",
    "    '0__double': 3220,\n",
    "    '0__double_outside': 3220,\n",
    "    '0__double_inside': 340,\n",
    "\n",
    "    'mm__double': 3334,\n",
    "    'mm__double_outside': 3333,\n",
    "    'mm__double_inside': 3333,\n",
    "    \n",
    "    '1__double': 3334,\n",
    "    '1__double_outside': 3333,\n",
    "    '1__double_inside': 3333,\n",
    "}\n",
    "cat_conc = 'norm_cat'\n",
    "for class_name in tqdm(class_list):\n",
    "    data_path = os.path.join('data', cat_conc, 'raw', f'{class_name}__all.csv')\n",
    "    data = pd.read_csv(data_path)\n",
    "    data['old_class'] = data['id'].apply(lambda x: x.split('-')[1])\n",
    "    sampled_data = pd.DataFrame()\n",
    "    for old_class in data['old_class'].unique().tolist():\n",
    "        print(old_class)\n",
    "        data_ = data[data['old_class'] == old_class].sample(n=data_size_for_tasks[old_class], random_state=random_seed)\n",
    "        sampled_data = pd.concat([sampled_data, data_], ignore_index=True)\n",
    "    print(class_name)\n",
    "    print(sampled_data.shape)\n",
    "    print(sampled_data.drop_duplicates('id').shape)\n",
    "    sampled_data.to_csv(os.path.join('data', cat_conc, 'raw', f'{class_name}__10k.csv'), index=False)\n",
    "        # break\n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add error\n",
    "def add_error(data:pd.DataFrame, random_seed=42, ignore_cols=['id', 'class', 'label', 'old_class', 'cat_conc']):\n",
    "    error_list = [0, 0.5, 1, 2]\n",
    "    data_ = data.copy()\n",
    "    for col in ignore_cols:\n",
    "        if col in data_.columns:\n",
    "            data_ = data_.drop(columns=[col])\n",
    "    random.seed(random_seed)\n",
    "    for i in range(len(data_)):\n",
    "        error = random.choice(error_list)\n",
    "        # print(error)\n",
    "        data_.loc[i] += [random.gauss(0, error / 100) for i in range(data_.shape[1])]\n",
    "    for col in ignore_cols:\n",
    "        if col in data.columns:\n",
    "            data_[col] = data[col]\n",
    "    # rearrange columns\n",
    "    cols = data.columns.tolist()\n",
    "    data_ = data_[cols]\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 34.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_cat (50000, 63)\n",
      "first: (10000, 63)\n",
      "second: (10000, 63)\n",
      "zero: (10000, 63)\n",
      "mm_1: (10000, 63)\n",
      "mm_2: (10000, 63)\n"
     ]
    }
   ],
   "source": [
    "# concat data, add error and split train_val test\n",
    "for cat_conc in ['norm_cat']:\n",
    "    data_root = os.path.join(f'data/{cat_conc}/raw/')\n",
    "    data = pd.DataFrame()\n",
    "    for task_name in tqdm(class_list):\n",
    "        data_path = os.path.join(data_root, f'{task_name}__10k.csv')\n",
    "        data_tmp = pd.read_csv(data_path)\n",
    "        # data_tmp.drop(columns=['old_class'], inplace=True)\n",
    "        data_tmp['class'] = task_name\n",
    "        data = pd.concat([data, data_tmp], axis=0, ignore_index=True)\n",
    "    print(cat_conc, data.shape)\n",
    "    data = add_error(data)\n",
    "    print('first:', data[data['class'] == 'first'].shape)\n",
    "    print('second:', data[data['class'] == 'second'].shape)\n",
    "    print('zero:', data[data['class'] == 'zero'].shape)\n",
    "    print('mm_1:', data[data['class'] == 'mm_1'].shape)\n",
    "    # print('mm_1_out:', data[data['class'] == 'mm_1_out'].shape)\n",
    "    print('mm_2:', data[data['class'] == 'mm_2'].shape)\n",
    "    # break\n",
    "    train_val_data, test_data = train_test_split(data, test_size=0.1, random_state=random_seed, stratify=data['class'])\n",
    "    train_val_data.to_csv(os.path.join('data', cat_conc, f'{cat_conc}_train_val.csv'), index=False)\n",
    "    test_data.to_csv(os.path.join('data', cat_conc, f'{cat_conc}_test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 63)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>s0</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>...</th>\n",
       "      <th>p22</th>\n",
       "      <th>p23</th>\n",
       "      <th>p24</th>\n",
       "      <th>p25</th>\n",
       "      <th>p26</th>\n",
       "      <th>p27</th>\n",
       "      <th>p28</th>\n",
       "      <th>p29</th>\n",
       "      <th>old_class</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>second-1__double-6886</td>\n",
       "      <td>1.014078</td>\n",
       "      <td>0.941292</td>\n",
       "      <td>0.874194</td>\n",
       "      <td>0.831934</td>\n",
       "      <td>0.796985</td>\n",
       "      <td>0.752483</td>\n",
       "      <td>0.710213</td>\n",
       "      <td>0.682713</td>\n",
       "      <td>0.618944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251490</td>\n",
       "      <td>0.255820</td>\n",
       "      <td>0.273534</td>\n",
       "      <td>0.269470</td>\n",
       "      <td>0.266760</td>\n",
       "      <td>0.267846</td>\n",
       "      <td>0.282937</td>\n",
       "      <td>0.281530</td>\n",
       "      <td>1__double</td>\n",
       "      <td>second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mm_2-mm__double-8713</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857185</td>\n",
       "      <td>0.740969</td>\n",
       "      <td>0.644123</td>\n",
       "      <td>0.562685</td>\n",
       "      <td>0.493658</td>\n",
       "      <td>0.434723</td>\n",
       "      <td>0.384089</td>\n",
       "      <td>0.340341</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303795</td>\n",
       "      <td>0.304273</td>\n",
       "      <td>0.304664</td>\n",
       "      <td>0.304983</td>\n",
       "      <td>0.305245</td>\n",
       "      <td>0.305460</td>\n",
       "      <td>0.305636</td>\n",
       "      <td>0.305781</td>\n",
       "      <td>mm__double</td>\n",
       "      <td>mm_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>first-0__double_outside-275</td>\n",
       "      <td>1.003719</td>\n",
       "      <td>0.943307</td>\n",
       "      <td>0.843800</td>\n",
       "      <td>0.748718</td>\n",
       "      <td>0.696003</td>\n",
       "      <td>0.690904</td>\n",
       "      <td>0.626570</td>\n",
       "      <td>0.550884</td>\n",
       "      <td>0.533442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400171</td>\n",
       "      <td>0.394868</td>\n",
       "      <td>0.370069</td>\n",
       "      <td>0.398233</td>\n",
       "      <td>0.393137</td>\n",
       "      <td>0.419216</td>\n",
       "      <td>0.387829</td>\n",
       "      <td>0.406436</td>\n",
       "      <td>0__double_outside</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>first-0__double_outside-6283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.876857</td>\n",
       "      <td>0.769023</td>\n",
       "      <td>0.674643</td>\n",
       "      <td>0.592040</td>\n",
       "      <td>0.519743</td>\n",
       "      <td>0.456460</td>\n",
       "      <td>0.401073</td>\n",
       "      <td>0.352590</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426240</td>\n",
       "      <td>0.429610</td>\n",
       "      <td>0.432611</td>\n",
       "      <td>0.435290</td>\n",
       "      <td>0.437688</td>\n",
       "      <td>0.439839</td>\n",
       "      <td>0.441774</td>\n",
       "      <td>0.443521</td>\n",
       "      <td>0__double_outside</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mm_2-mm__double_outside-288</td>\n",
       "      <td>0.992080</td>\n",
       "      <td>0.954080</td>\n",
       "      <td>0.862516</td>\n",
       "      <td>0.828265</td>\n",
       "      <td>0.802843</td>\n",
       "      <td>0.746227</td>\n",
       "      <td>0.698473</td>\n",
       "      <td>0.675103</td>\n",
       "      <td>0.621021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.237389</td>\n",
       "      <td>0.260133</td>\n",
       "      <td>0.251989</td>\n",
       "      <td>0.260898</td>\n",
       "      <td>0.263738</td>\n",
       "      <td>0.270799</td>\n",
       "      <td>0.270729</td>\n",
       "      <td>0.291732</td>\n",
       "      <td>mm__double_outside</td>\n",
       "      <td>mm_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id        s0        s1        s2        s3  \\\n",
       "0         second-1__double-6886  1.014078  0.941292  0.874194  0.831934   \n",
       "1          mm_2-mm__double-8713  1.000000  0.857185  0.740969  0.644123   \n",
       "2   first-0__double_outside-275  1.003719  0.943307  0.843800  0.748718   \n",
       "3  first-0__double_outside-6283  1.000000  0.876857  0.769023  0.674643   \n",
       "4   mm_2-mm__double_outside-288  0.992080  0.954080  0.862516  0.828265   \n",
       "\n",
       "         s4        s5        s6        s7        s8  ...       p22       p23  \\\n",
       "0  0.796985  0.752483  0.710213  0.682713  0.618944  ...  0.251490  0.255820   \n",
       "1  0.562685  0.493658  0.434723  0.384089  0.340341  ...  0.303795  0.304273   \n",
       "2  0.696003  0.690904  0.626570  0.550884  0.533442  ...  0.400171  0.394868   \n",
       "3  0.592040  0.519743  0.456460  0.401073  0.352590  ...  0.426240  0.429610   \n",
       "4  0.802843  0.746227  0.698473  0.675103  0.621021  ...  0.237389  0.260133   \n",
       "\n",
       "        p24       p25       p26       p27       p28       p29  \\\n",
       "0  0.273534  0.269470  0.266760  0.267846  0.282937  0.281530   \n",
       "1  0.304664  0.304983  0.305245  0.305460  0.305636  0.305781   \n",
       "2  0.370069  0.398233  0.393137  0.419216  0.387829  0.406436   \n",
       "3  0.432611  0.435290  0.437688  0.439839  0.441774  0.443521   \n",
       "4  0.251989  0.260898  0.263738  0.270799  0.270729  0.291732   \n",
       "\n",
       "            old_class   class  \n",
       "0           1__double  second  \n",
       "1          mm__double    mm_2  \n",
       "2   0__double_outside   first  \n",
       "3   0__double_outside   first  \n",
       "4  mm__double_outside    mm_2  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/norm_cat/norm_cat_train_val.csv')\n",
    "# df[df['id'].str.contains('0__single_')].shape\n",
    "# df[df['class'] == 'second'].shape\n",
    "print(df.shape)\n",
    "df.head()\n",
    "# df[df['id'].str.contains('add')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@set_property(\"fctype\", \"combiner\")\n",
    "def catch22(x, param):\n",
    "    \"\"\"\n",
    "    pycatch22\n",
    "\n",
    "    :param x: the time series to calculate the feature of\n",
    "    :type x: pandas.Series\n",
    "    :return: list of tuples (s, f) where s are the feature name in catch22, serialized as a string,\n",
    "             and f the respective feature value as bool, int or float\n",
    "    :return type: pandas.Series\n",
    "    \"\"\"\n",
    "    data = pycatch22.catch22_all(x)\n",
    "\n",
    "    return [(name, value) for name, value in zip(data['names'], data['values'])]\n",
    "\n",
    "settings = ComprehensiveFCParameters()\n",
    "settings[catch22] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_val: 100%|██████████| 45000/45000 [00:08<00:00, 5071.29it/s]\n",
      "test: 100%|██████████| 5000/5000 [00:00<00:00, 5109.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# fit dataset format for tsfresh\n",
    "for cat_conc in ['norm_cat']:\n",
    "    data_root = os.path.join(f'data/{cat_conc}')\n",
    "    for set_ in ['train_val', 'test']:\n",
    "        data_path = os.path.join(data_root, f'{cat_conc}_{set_}.csv')\n",
    "        data = pd.read_csv(data_path)\n",
    "        data.drop(columns=['old_class'], inplace=True)\n",
    "        data_tsfresh = pd.DataFrame(columns=['id', 't', 's', 'p'])\n",
    "        data_tsfresh_ids = []\n",
    "        data_tsfresh_ts = []\n",
    "        data_tsfresh_ss = []\n",
    "        data_tsfresh_ps = []\n",
    "        for i in tqdm(range(data.shape[0]), desc=set_):\n",
    "            id = data.loc[i, 'id']\n",
    "            for j in range(30):\n",
    "                data_tsfresh_ss.append(data.loc[i, f's{j}'])\n",
    "                data_tsfresh_ps.append(data.loc[i, f'p{j}'])\n",
    "            data_tsfresh_ids += [id] * 30\n",
    "            data_tsfresh_ts += list(range(30))\n",
    "        data_tsfresh['id'] = data_tsfresh_ids\n",
    "        data_tsfresh['t'] = data_tsfresh_ts\n",
    "        data_tsfresh['s'] = data_tsfresh_ss\n",
    "        data_tsfresh['p'] = data_tsfresh_ps\n",
    "        data_tsfresh.to_csv(os.path.join(data_root, f'{cat_conc}_{set_}_tsfresh.csv'), index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_cat train_val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 60/60 [03:09<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_cat test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 60/60 [00:20<00:00,  2.91it/s]\n"
     ]
    }
   ],
   "source": [
    "for cat_conc in [\n",
    "    'norm_cat',\n",
    "]:\n",
    "    data_root = os.path.join(f'data/{cat_conc}')\n",
    "    for set_ in ['train_val', 'test']:\n",
    "        print(cat_conc, set_)\n",
    "        data_path = os.path.join(data_root, f'{cat_conc}_{set_}_tsfresh.csv')\n",
    "        data = pd.read_csv(data_path)\n",
    "        data_feat = extract_features(data, column_id='id', column_sort='t', default_fc_parameters=settings)\n",
    "        data_feat['id'] = data_feat.index\n",
    "        data_feat['class'] = data_feat['id'].apply(lambda x: x.split('-')[0])\n",
    "        data_feat.to_csv(os.path.join(data_root, f'{cat_conc}_{set_}_tsfresh_feat.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm_cat train_val\n",
      "norm_cat test\n"
     ]
    }
   ],
   "source": [
    "for cat_conc in [\n",
    "    'norm_cat',\n",
    "]:\n",
    "    data_root = os.path.join(f'data/{cat_conc}')\n",
    "    for set_ in ['train_val', 'test']:\n",
    "        print(cat_conc, set_)\n",
    "        data_path = os.path.join(data_root, f'{cat_conc}_{set_}.csv')\n",
    "        data = pd.read_csv(data_path)\n",
    "        # data.drop(columns=['old_class'], inplace=True)\n",
    "        # data.to_csv(os.path.join(data_root, f'{cat_conc}_{set_}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_org_mech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
