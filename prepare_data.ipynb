{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.feature_extraction.settings import ComprehensiveFCParameters\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction.feature_calculators import set_property\n",
    "import pycatch22\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(task_names, class_num, detailed_class2class):\n",
    "    \"\"\"\n",
    "    collect data from raw json data and save them into csv files\n",
    "    \"\"\"\n",
    "    data_root = os.path.join('data', 'ode_raw_data')\n",
    "    for task_name in tqdm(task_names):\n",
    "        task_data_root = os.path.join(data_root, task_name)\n",
    "        save_root = os.path.join('data', f'{str(class_num)}_class', 'raw')\n",
    "        if not os.path.exists(save_root):\n",
    "            os.makedirs(save_root)\n",
    "\n",
    "        class_name = detailed_class2class[task_name]\n",
    "        new_class_data_path = os.path.join(save_root, f'{class_name}__all.csv')\n",
    "        if os.path.exists(new_class_data_path):\n",
    "            df_new_class = pd.read_csv(new_class_data_path)\n",
    "        else:\n",
    "            df_new_class = pd.DataFrame()\n",
    "\n",
    "        data_column = ['id']\n",
    "        for i in range(30):\n",
    "            data_column += [f's{i}']\n",
    "        for i in range(30):\n",
    "            data_column += [f'p{i}']\n",
    "        data = pd.DataFrame(columns=data_column)\n",
    "\n",
    "        for file in os.listdir(task_data_root):\n",
    "            if file.endswith('.json'):\n",
    "                with open(os.path.join(task_data_root, file), 'r') as f:\n",
    "                    task_data = json.load(f)\n",
    "                \n",
    "                id = f'{class_name}-{task_name}-{file[:-5]}'\n",
    "                data.loc[data.shape[0]] = [id] + task_data['s'] + task_data['p']\n",
    "\n",
    "        data_noDup = data.drop_duplicates('id')\n",
    "        assert data_noDup.shape[0] == data.shape[0]\n",
    "        print(f'{task_name} has {data.shape[0]} samples')\n",
    "\n",
    "        df_new_class = pd.concat([df_new_class, data], ignore_index=True)\n",
    "        df_new_class.to_csv(new_class_data_path, index=False)\n",
    "\n",
    "\n",
    "# add error to data\n",
    "def add_error(data:pd.DataFrame, random_seed=42, ignore_cols=['id', 'class', 'label', 'old_class', 'cat_conc']):\n",
    "    \"\"\"\n",
    "    add 0%, 0.5%, 1% or 2% gaussian noise to data\n",
    "    \"\"\"\n",
    "    error_list = [0, 0.5, 1, 2]\n",
    "    data_ = data.copy()\n",
    "    for col in ignore_cols:\n",
    "        if col in data_.columns:\n",
    "            data_ = data_.drop(columns=[col])\n",
    "    random.seed(random_seed)\n",
    "    for i in range(len(data_)):\n",
    "        error = random.choice(error_list)\n",
    "        # print(error)\n",
    "        data_.loc[i] += [random.gauss(0, error / 100) for i in range(data_.shape[1])]\n",
    "    for col in ignore_cols:\n",
    "        if col in data.columns:\n",
    "            data_[col] = data[col]\n",
    "    # rearrange columns\n",
    "    cols = data.columns.tolist()\n",
    "    data_ = data_[cols]\n",
    "    return data_\n",
    "\n",
    "\n",
    "# add catch22 features into tsfresh\n",
    "@set_property(\"fctype\", \"combiner\")\n",
    "def catch22(x, param):\n",
    "    \"\"\"\n",
    "    pycatch22, a Python library that wraps the CATCH time series characteristics\n",
    "\n",
    "    Lubba et al. catch22: CAnonical Time-series CHaracteristics, Data Min Knowl Disc 33, 1821 (2019).\n",
    "\n",
    "    :param x: the time series to calculate the feature of\n",
    "    :type x: pandas.Series\n",
    "    :return: list of tuples (s, f) where s are the feature name in catch22, serialized as a string,\n",
    "             and f the respective feature value as bool, int or float\n",
    "    :return type: pandas.Series\n",
    "    \"\"\"\n",
    "    data = pycatch22.catch22_all(x)\n",
    "\n",
    "    return [(name, value) for name, value in zip(data['names'], data['values'])]\n",
    "\n",
    "settings = ComprehensiveFCParameters()\n",
    "settings[catch22] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. prepare data for 5 classes\n",
    "- collect data from ode raw data(json)\n",
    "- add gaussian noise to the data\n",
    "- split the data into train and test set\n",
    "- extract tsfresh features from the data and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 5\n",
    "\n",
    "detailed_class_list = [\n",
    "    '1__single', \n",
    "    '1__double',\n",
    "    '1__double_outside',\n",
    "    '1__double_inside',\n",
    "    '0__single',\n",
    "    '0__double',\n",
    "    '0__double_outside',\n",
    "    '0__double_inside',\n",
    "    'mm__single',\n",
    "    # 'mm__single_outside',\n",
    "    'mm__double',\n",
    "    'mm__double_outside',\n",
    "    'mm__double_inside',\n",
    "]\n",
    "\n",
    "detailed_class_short_list = [\n",
    "    '1_sgl', \n",
    "    '1_dbl',\n",
    "    '1_dbl_out',\n",
    "    '1_dbl_in',\n",
    "    '0_sgl',\n",
    "    '0_dbl',\n",
    "    '0_dbl_out',\n",
    "    '0_dbl_in',\n",
    "    'mm_sgl',\n",
    "    # 'mm_sgl_out',\n",
    "    'mm_dbl',\n",
    "    'mm_dbl_out',\n",
    "    'mm_dbl_in',\n",
    "]\n",
    "\n",
    "detailed_class2class = {\n",
    "    '1__single': 'first', \n",
    "    '1__double': 'second',\n",
    "    '1__double_outside': 'second',\n",
    "    '1__double_inside': 'second',\n",
    "    '0__single': 'zero',\n",
    "    '0__double': 'first',\n",
    "    '0__double_outside': 'first',\n",
    "    '0__double_inside': 'first',\n",
    "    'mm__single': 'mm_1',\n",
    "    # 'mm__single_outside': 'mm_1_out',\n",
    "    'mm__double': 'mm_2',\n",
    "    'mm__double_outside': 'mm_2',\n",
    "    'mm__double_inside': 'mm_2',\n",
    "}\n",
    "\n",
    "class_list = [\n",
    "    'zero',\n",
    "    'first',\n",
    "    'second',\n",
    "    'mm_1',\n",
    "    # 'mm_1_out',\n",
    "    'mm_2',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/12 [00:22<04:09, 22.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__single has 11404 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2/12 [00:44<03:42, 22.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__double has 10710 samples\n",
      "1__double_outside has 11156 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 3/12 [01:06<03:17, 21.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__double_inside has 13332 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 4/12 [01:34<03:14, 24.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__single has 13671 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 5/12 [01:59<02:53, 24.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__double has 12214 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 6/12 [02:21<02:23, 23.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__double_outside has 20369 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 7/12 [03:06<02:33, 30.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__double_inside has 12902 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9/12 [03:49<01:16, 25.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__single has 10742 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 10/12 [04:08<00:46, 23.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__double has 10728 samples\n",
      "mm__double_outside has 10772 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 11/12 [04:27<00:22, 22.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__double_inside has 11022 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [04:46<00:00, 23.91s/it]\n"
     ]
    }
   ],
   "source": [
    "collect_data(\n",
    "    task_names=detailed_class_list, \n",
    "    class_num=class_num,\n",
    "    detailed_class2class=detailed_class2class\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__single\n",
      "zero\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:00<00:01,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__single\n",
      "0__double\n",
      "0__double_outside\n",
      "0__double_inside\n",
      "first\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:01<00:01,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__double\n",
      "1__double_outside\n",
      "1__double_inside\n",
      "second\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:01<00:01,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__single\n",
      "mm_1\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:01<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__double\n",
      "mm__double_outside\n",
      "mm__double_inside\n",
      "mm_2\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.05it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_size_for_tasks = {\n",
    "    '0__single': 10000,\n",
    "\n",
    "    'mm__single': 10000,\n",
    "\n",
    "    # 'mm__single_outside': 10000,\n",
    "\n",
    "    '1__single': 3220, \n",
    "    '0__double': 3220,\n",
    "    '0__double_outside': 3220,\n",
    "    '0__double_inside': 340,\n",
    "\n",
    "    'mm__double': 3334,\n",
    "    'mm__double_outside': 3333,\n",
    "    'mm__double_inside': 3333,\n",
    "    \n",
    "    '1__double': 3334,\n",
    "    '1__double_outside': 3333,\n",
    "    '1__double_inside': 3333,\n",
    "}\n",
    "random_seed = 29\n",
    "for class_name in tqdm(class_list):\n",
    "    data_path = os.path.join('data', f'{str(class_num)}_class', 'raw', f'{class_name}__all.csv')\n",
    "    data = pd.read_csv(data_path)\n",
    "    data['old_class'] = data['id'].apply(lambda x: x.split('-')[1])\n",
    "    sampled_data = pd.DataFrame()\n",
    "    for old_class in data['old_class'].unique().tolist():\n",
    "        print(old_class)\n",
    "        data_ = data[data['old_class'] == old_class].sample(n=data_size_for_tasks[old_class], random_state=random_seed)\n",
    "        sampled_data = pd.concat([sampled_data, data_], ignore_index=True)\n",
    "    print(class_name)\n",
    "    print(sampled_data.shape)\n",
    "    print(sampled_data.drop_duplicates('id').shape)\n",
    "    sampled_data.to_csv(os.path.join('data', f'{str(class_num)}_class', 'raw', f'{class_name}__10k.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 19.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first: (10000, 63)\n",
      "second: (10000, 63)\n",
      "zero: (10000, 63)\n",
      "mm_1: (10000, 63)\n",
      "mm_2: (10000, 63)\n"
     ]
    }
   ],
   "source": [
    "# concat data, add error and split train_val test\n",
    "data_root = os.path.join(f'data', f'{str(class_num)}_class', 'raw')\n",
    "data = pd.DataFrame()\n",
    "for task_name in tqdm(class_list):\n",
    "    data_path = os.path.join(data_root, f'{task_name}__10k.csv')\n",
    "    data_tmp = pd.read_csv(data_path)\n",
    "    data_tmp['class'] = task_name\n",
    "    data = pd.concat([data, data_tmp], axis=0, ignore_index=True)\n",
    "data = add_error(data)\n",
    "print('first:', data[data['class'] == 'first'].shape)\n",
    "print('second:', data[data['class'] == 'second'].shape)\n",
    "print('zero:', data[data['class'] == 'zero'].shape)\n",
    "print('mm_1:', data[data['class'] == 'mm_1'].shape)\n",
    "# print('mm_1_out:', data[data['class'] == 'mm_1_out'].shape)\n",
    "print('mm_2:', data[data['class'] == 'mm_2'].shape)\n",
    "\n",
    "train_val_data, test_data = train_test_split(data, test_size=0.1, random_state=random_seed, stratify=data['class'])\n",
    "train_val_data.to_csv(os.path.join('data', f'{str(class_num)}_class', 'train_val.csv'), index=False)\n",
    "test_data.to_csv(os.path.join('data', f'{str(class_num)}_class', 'test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_val: 100%|██████████| 45000/45000 [00:16<00:00, 2778.15it/s]\n",
      "Feature Extraction: 100%|██████████| 60/60 [06:19<00:00,  6.33s/it] \n",
      "test: 100%|██████████| 5000/5000 [00:01<00:00, 3415.76it/s]\n",
      "Feature Extraction: 100%|██████████| 60/60 [00:30<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "data_root = os.path.join('data', f'{str(class_num)}_class')\n",
    "for set_ in ['train_val', 'test']:\n",
    "    data_path = os.path.join(data_root, f'{set_}.csv')\n",
    "    data = pd.read_csv(data_path)\n",
    "    data.drop(columns=['old_class'], inplace=True)\n",
    "    \n",
    "    # fit dataset format for tsfresh\n",
    "    data_tsfresh = pd.DataFrame(columns=['id', 't', 's', 'p'])\n",
    "    data_tsfresh_ids = []\n",
    "    data_tsfresh_ts = []\n",
    "    data_tsfresh_ss = []\n",
    "    data_tsfresh_ps = []\n",
    "    for i in tqdm(range(data.shape[0]), desc=set_):\n",
    "        id = data.loc[i, 'id']\n",
    "        for j in range(30):\n",
    "            data_tsfresh_ss.append(data.loc[i, f's{j}'])\n",
    "            data_tsfresh_ps.append(data.loc[i, f'p{j}'])\n",
    "        data_tsfresh_ids += [id] * 30\n",
    "        data_tsfresh_ts += list(range(30))\n",
    "    data_tsfresh['id'] = data_tsfresh_ids\n",
    "    data_tsfresh['t'] = data_tsfresh_ts\n",
    "    data_tsfresh['s'] = data_tsfresh_ss\n",
    "    data_tsfresh['p'] = data_tsfresh_ps\n",
    "\n",
    "    # extract tsfresh features\n",
    "    data_feat = extract_features(data_tsfresh, column_id='id', column_sort='t', default_fc_parameters=settings)\n",
    "    data_feat['id'] = data_feat.index\n",
    "    data_feat['class'] = data_feat['id'].apply(lambda x: x.split('-')[0])\n",
    "    data_feat.to_csv(os.path.join(data_root, f'{set_}_tsfresh_feat.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. prepare data for 6 classes\n",
    "- collect data from ode raw data(json)\n",
    "- add gaussian noise to the data\n",
    "- split the data into train and test set\n",
    "- extract tsfresh features from the data and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 6\n",
    "\n",
    "detailed_class_list = [\n",
    "    '1__single', \n",
    "    '1__double',\n",
    "    '1__double_outside',\n",
    "    '1__double_inside',\n",
    "    '0__single',\n",
    "    '0__double',\n",
    "    '0__double_outside',\n",
    "    '0__double_inside',\n",
    "    'mm__single',\n",
    "    'mm__single_outside',\n",
    "    'mm__double',\n",
    "    'mm__double_outside',\n",
    "    'mm__double_inside',\n",
    "]\n",
    "\n",
    "detailed_class_short_list = [\n",
    "    '1_sgl', \n",
    "    '1_dbl',\n",
    "    '1_dbl_out',\n",
    "    '1_dbl_in',\n",
    "    '0_sgl',\n",
    "    '0_dbl',\n",
    "    '0_dbl_out',\n",
    "    '0_dbl_in',\n",
    "    'mm_sgl',\n",
    "    'mm_sgl_out',\n",
    "    'mm_dbl',\n",
    "    'mm_dbl_out',\n",
    "    'mm_dbl_in',\n",
    "]\n",
    "\n",
    "detailed_class2class = {\n",
    "    '1__single': 'first', \n",
    "    '1__double': 'second',\n",
    "    '1__double_outside': 'second',\n",
    "    '1__double_inside': 'second',\n",
    "    '0__single': 'zero',\n",
    "    '0__double': 'first',\n",
    "    '0__double_outside': 'first',\n",
    "    '0__double_inside': 'first',\n",
    "    'mm__single': 'mm_1',\n",
    "    'mm__single_outside': 'mm_1_out',\n",
    "    'mm__double': 'mm_2',\n",
    "    'mm__double_outside': 'mm_2',\n",
    "    'mm__double_inside': 'mm_2',\n",
    "}\n",
    "\n",
    "class_list = [\n",
    "    'zero',\n",
    "    'first',\n",
    "    'second',\n",
    "    'mm_1',\n",
    "    'mm_1_out',\n",
    "    'mm_2',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__single has 11404 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/13 [00:35<07:04, 35.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__double has 10710 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 2/13 [01:12<06:37, 36.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__double_outside has 11156 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 3/13 [01:55<06:36, 39.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__double_inside has 13332 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 4/13 [02:44<06:30, 43.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__single has 13671 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 5/13 [03:11<04:59, 37.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__double has 12214 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 6/13 [03:31<03:40, 31.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__double_outside has 20369 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 7/13 [04:17<03:35, 35.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__double_inside has 12902 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 9/13 [05:03<01:55, 28.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__single has 10742 samples\n",
      "mm__single_outside has 19319 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 11/13 [06:08<00:58, 29.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__double has 10728 samples\n",
      "mm__double_outside has 10772 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 12/13 [06:28<00:26, 26.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__double_inside has 11022 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [06:48<00:00, 31.42s/it]\n"
     ]
    }
   ],
   "source": [
    "collect_data(\n",
    "    task_names=detailed_class_list, \n",
    "    class_num=class_num,\n",
    "    detailed_class2class=detailed_class2class\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:00<00:01,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0__single\n",
      "zero\n",
      "(10000, 62)\n",
      "(10000, 62)\n",
      "1__single\n",
      "0__double\n",
      "0__double_outside\n",
      "0__double_inside\n",
      "first\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:00<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1__double\n",
      "1__double_outside\n",
      "1__double_inside\n",
      "second\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [00:01<00:00,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__single\n",
      "mm_1\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [00:01<00:00,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__single_outside\n",
      "mm_1_out\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mm__double\n",
      "mm__double_outside\n",
      "mm__double_inside\n",
      "mm_2\n",
      "(10000, 62)\n",
      "(10000, 62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_size_for_tasks = {\n",
    "    '0__single': 10000,\n",
    "\n",
    "    'mm__single': 10000,\n",
    "\n",
    "    'mm__single_outside': 10000,\n",
    "\n",
    "    '1__single': 3220, \n",
    "    '0__double': 3220,\n",
    "    '0__double_outside': 3220,\n",
    "    '0__double_inside': 340,\n",
    "\n",
    "    'mm__double': 3334,\n",
    "    'mm__double_outside': 3333,\n",
    "    'mm__double_inside': 3333,\n",
    "    \n",
    "    '1__double': 3334,\n",
    "    '1__double_outside': 3333,\n",
    "    '1__double_inside': 3333,\n",
    "}\n",
    "random_seed = 29\n",
    "for class_name in tqdm(class_list):\n",
    "    data_path = os.path.join('data', f'{str(class_num)}_class', 'raw', f'{class_name}__all.csv')\n",
    "    data = pd.read_csv(data_path)\n",
    "    data['old_class'] = data['id'].apply(lambda x: x.split('-')[1])\n",
    "    sampled_data = pd.DataFrame()\n",
    "    for old_class in data['old_class'].unique().tolist():\n",
    "        print(old_class)\n",
    "        data_ = data[data['old_class'] == old_class].sample(n=data_size_for_tasks[old_class], random_state=random_seed)\n",
    "        sampled_data = pd.concat([sampled_data, data_], ignore_index=True)\n",
    "    print(class_name)\n",
    "    print(sampled_data.shape)\n",
    "    print(sampled_data.drop_duplicates('id').shape)\n",
    "    sampled_data.to_csv(os.path.join('data', f'{str(class_num)}_class', 'raw', f'{class_name}__10k.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 33.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first: (10000, 63)\n",
      "second: (10000, 63)\n",
      "zero: (10000, 63)\n",
      "mm_1: (10000, 63)\n",
      "mm_1_out: (10000, 63)\n",
      "mm_2: (10000, 63)\n"
     ]
    }
   ],
   "source": [
    "# concat data, add error and split train_val test\n",
    "data_root = os.path.join(f'data', f'{str(class_num)}_class', 'raw')\n",
    "data = pd.DataFrame()\n",
    "for task_name in tqdm(class_list):\n",
    "    data_path = os.path.join(data_root, f'{task_name}__10k.csv')\n",
    "    data_tmp = pd.read_csv(data_path)\n",
    "    data_tmp['class'] = task_name\n",
    "    data = pd.concat([data, data_tmp], axis=0, ignore_index=True)\n",
    "data = add_error(data)\n",
    "print('first:', data[data['class'] == 'first'].shape)\n",
    "print('second:', data[data['class'] == 'second'].shape)\n",
    "print('zero:', data[data['class'] == 'zero'].shape)\n",
    "print('mm_1:', data[data['class'] == 'mm_1'].shape)\n",
    "print('mm_1_out:', data[data['class'] == 'mm_1_out'].shape)\n",
    "print('mm_2:', data[data['class'] == 'mm_2'].shape)\n",
    "\n",
    "train_val_data, test_data = train_test_split(data, test_size=0.1, random_state=random_seed, stratify=data['class'])\n",
    "train_val_data.to_csv(os.path.join('data', f'{str(class_num)}_class', 'train_val.csv'), index=False)\n",
    "test_data.to_csv(os.path.join('data', f'{str(class_num)}_class', 'test.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_val: 100%|██████████| 54000/54000 [00:10<00:00, 5114.72it/s]\n",
      "Feature Extraction: 100%|██████████| 60/60 [04:06<00:00,  4.12s/it]\n",
      "test: 100%|██████████| 6000/6000 [00:01<00:00, 4213.29it/s]\n",
      "Feature Extraction: 100%|██████████| 60/60 [00:34<00:00,  1.74it/s]\n"
     ]
    }
   ],
   "source": [
    "data_root = os.path.join('data', f'{str(class_num)}_class')\n",
    "for set_ in ['train_val', 'test']:\n",
    "    data_path = os.path.join(data_root, f'{set_}.csv')\n",
    "    data = pd.read_csv(data_path)\n",
    "    data.drop(columns=['old_class'], inplace=True)\n",
    "    \n",
    "    # fit dataset format for tsfresh\n",
    "    data_tsfresh = pd.DataFrame(columns=['id', 't', 's', 'p'])\n",
    "    data_tsfresh_ids = []\n",
    "    data_tsfresh_ts = []\n",
    "    data_tsfresh_ss = []\n",
    "    data_tsfresh_ps = []\n",
    "    for i in tqdm(range(data.shape[0]), desc=set_):\n",
    "        id = data.loc[i, 'id']\n",
    "        for j in range(30):\n",
    "            data_tsfresh_ss.append(data.loc[i, f's{j}'])\n",
    "            data_tsfresh_ps.append(data.loc[i, f'p{j}'])\n",
    "        data_tsfresh_ids += [id] * 30\n",
    "        data_tsfresh_ts += list(range(30))\n",
    "    data_tsfresh['id'] = data_tsfresh_ids\n",
    "    data_tsfresh['t'] = data_tsfresh_ts\n",
    "    data_tsfresh['s'] = data_tsfresh_ss\n",
    "    data_tsfresh['p'] = data_tsfresh_ps\n",
    "\n",
    "    # extract tsfresh features\n",
    "    data_feat = extract_features(data_tsfresh, column_id='id', column_sort='t', default_fc_parameters=settings)\n",
    "    data_feat['id'] = data_feat.index\n",
    "    data_feat['class'] = data_feat['id'].apply(lambda x: x.split('-')[0])\n",
    "    data_feat.to_csv(os.path.join(data_root, f'{set_}_tsfresh_feat.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_org_mech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
